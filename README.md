# ğŸ¯ RAG de AnÃ¡lisis de Riesgos

Sistema de consulta inteligente sobre documentos de riesgos usando arquitectura **Retrieval-Augmented Generation (RAG)** con enfoque baseline-first, robusto y reproducible.

DiseÃ±ado para funcionar **sin dependencias de modelos generativos** y escalar opcionalmente a LLMs, manteniendo control, trazabilidad y estabilidad en producciÃ³n.

---

## ğŸ“‹ Tabla de Contenidos

- [Objetivo](#-objetivo)
- [CaracterÃ­sticas Principales](#-caracterÃ­sticas-principales)
- [Arquitectura](#-arquitectura)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [ConfiguraciÃ³n](#-configuraciÃ³n)
- [Uso](#-uso)
- [Modos de OperaciÃ³n](#-modos-de-operaciÃ³n)
- [API Reference](#-api-reference)
- [Roadmap](#-roadmap)
- [ContribuciÃ³n](#-contribuciÃ³n)

---

## ğŸ¯ Objetivo

Permitir consultas en lenguaje natural sobre documentos de riesgos (PDFs), entregando:

- âœ… **Respuestas claras y justificadas** con contexto relevante
- ğŸ“„ **Evidencia textual explÃ­cita** con referencias a fuentes
- ğŸ›¡ï¸ **Comportamiento estable** incluso ante fallos de APIs externas
- ğŸ” **Trazabilidad completa** de cada respuesta generada

Este proyecto prioriza **ingenierÃ­a de sistemas de IA en producciÃ³n**, no solo experimentaciÃ³n.

---

## â­ CaracterÃ­sticas Principales

### ğŸ—ï¸ Arquitectura Resiliente
- **Modo Baseline** (predeterminado): BM25 + extracciÃ³n extractiva sin uso de tokens
- **Modo LLM** (opcional): Gemini + embeddings semÃ¡nticos con fallback automÃ¡tico
- **DegradaciÃ³n elegante**: Si LLM falla, el sistema continÃºa funcionando en modo baseline

### ğŸ”’ ProducciÃ³n-Ready
- Zero downtime por cuotas de API
- Respuestas determinÃ­sticas y reproducibles
- Logging estructurado y mÃ©tricas de rendimiento
- Manejo robusto de errores

### ğŸ“Š Transparencia
- Fuentes citadas explÃ­citamente
- Scores de relevancia por fragmento
- Metadata de cada respuesta (modo usado, latencia, chunks recuperados)

---

## ğŸ§  Arquitectura
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PDFs   â”‚ â”€â”€â–¶ â”‚ Ingesta â”‚ â”€â”€â–¶ â”‚ Indexing â”‚ â”€â”€â–¶ â”‚ RAG â”‚ â”€â”€â–¶ â”‚ Frontend â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚              â”‚
                                       â–¼              â–¼
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚   BM25   â”‚   â”‚ Chroma  â”‚
                                 â”‚ Baseline â”‚   â”‚  (LLM)  â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de Consulta

1. **Usuario** envÃ­a pregunta en lenguaje natural
2. **Query Wrapper** determina modo (baseline/LLM)
3. **Retriever** obtiene chunks relevantes (BM25 o vectorial)
4. **Generator** produce respuesta (extractiva o generativa)
5. **API** devuelve respuesta + fuentes + metadata

---

## ğŸ—‚ï¸ Estructura del Proyecto
```
RAG_riesgos/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                # FastAPI server + endpoints
â”‚   â”œâ”€â”€ query_wrapper.py       # Orquestador RAG (modo selector)
â”‚   â”œâ”€â”€ baseline_rag.py        # Motor extractivo (BM25)
â”‚   â”œâ”€â”€ baseline_store.py      # Ãndice BM25 + persistencia
â”‚   â”œâ”€â”€ Pdf_loader.py          # Procesamiento y chunking de PDFs
â”‚   â”œâ”€â”€ Vector_store.py        # Embeddings + ChromaDB (modo LLM)
â”‚   â””â”€â”€ config.py              # ConfiguraciÃ³n centralizada
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.jsx
â”‚   â”‚   â”‚   â””â”€â”€ SourcesPanel.jsx
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â””â”€â”€ index.js
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Doc chatbot.pdf        # Documento(s) fuente
â”‚
â”œâ”€â”€ chroma_db_riesgos/         # Vector DB (solo modo LLM)
â”œâ”€â”€ baseline_index/            # Ãndice BM25 persistido
â”‚
â”œâ”€â”€ tests/                     # Tests unitarios + integraciÃ³n
â”œâ”€â”€ .env.example
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ InstalaciÃ³n

### Prerrequisitos

- Python 3.11+
- Node.js 18+ y npm
- (Opcional) Docker y Docker Compose

### Backend
```bash
# Clonar repositorio
git clone https://github.com/tu-usuario/RAG_riesgos.git
cd RAG_riesgos

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Configurar variables de entorno
cp .env.example .env
# Editar .env con tu configuraciÃ³n
```

### Frontend
```bash
cd frontend
npm install
```

---

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno (`.env`)
```bash
# Modo de operaciÃ³n (baseline | llm)
RAG_MODE=baseline

# API Keys (solo para modo LLM)
GOOGLE_API_KEY=tu_api_key_aqui

# ConfiguraciÃ³n de chunking
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# ConfiguraciÃ³n de retrieval
TOP_K=5
MIN_SCORE=0.3

# ConfiguraciÃ³n de servidor
BACKEND_PORT=8000
FRONTEND_PORT=3000
```

---

## ğŸ® Uso

### Iniciar Backend
```bash
# Desarrollo
uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000

# ProducciÃ³n
uvicorn backend.main:app --workers 4 --host 0.0.0.0 --port 8000
```

La API estarÃ¡ disponible en:
- **AplicaciÃ³n**: `http://localhost:8000`
- **DocumentaciÃ³n interactiva**: `http://localhost:8000/docs`
- **OpenAPI Schema**: `http://localhost:8000/openapi.json`

### Iniciar Frontend
```bash
cd frontend
npm start
```

Interfaz disponible en: `http://localhost:3000`

### Docker (Recomendado para ProducciÃ³n)
```bash
# Construir y levantar servicios
docker-compose up --build

# Solo backend
docker-compose up backend

# Detener servicios
docker-compose down
```

---

## ğŸ”§ Modos de OperaciÃ³n

### 1ï¸âƒ£ Modo Baseline (Predeterminado)

**CaracterÃ­sticas:**
- BÃºsqueda lÃ©xica con BM25
- ExtracciÃ³n de frases relevantes
- **Cero consumo de tokens**
- Latencia < 100ms
- 100% reproducible

**CuÃ¡ndo usar:**
- Entornos de producciÃ³n estables
- Cumplimiento regulatorio estricto
- Restricciones de presupuesto
- Documentos tÃ©cnicos/legales donde la cita exacta es crÃ­tica

**ActivaciÃ³n:**
```bash
export RAG_MODE=baseline
```

### 2ï¸âƒ£ Modo LLM (Opcional)

**CaracterÃ­sticas:**
- Embeddings semÃ¡nticos (Google Gemini)
- BÃºsqueda vectorial con ChromaDB
- Respuestas generativas contextuales
- **Fallback automÃ¡tico** a baseline si hay errores

**CuÃ¡ndo usar:**
- Consultas complejas que requieren sÃ­ntesis
- Usuarios no tÃ©cnicos
- Disponibilidad de presupuesto para APIs

**ActivaciÃ³n:**
```bash
export RAG_MODE=llm
export GOOGLE_API_KEY=tu_api_key
```

**Manejo de Errores:**
```
LLM Request
    â”‚
    â”œâ”€ Success â”€â”€â–¶ Respuesta generativa
    â”‚
    â””â”€ Error (429/Quota/Network)
            â”‚
            â””â”€â”€â–¶ Automatic Fallback â”€â”€â–¶ Baseline Response
```

---

## ğŸ“¡ API Reference

### `POST /query`

Procesa una consulta en lenguaje natural.

**Request:**
```json
{
  "question": "Â¿CuÃ¡les son los riesgos de liquidez?",
  "mode": "auto"
}
```

**Response:**
```json
{
  "answer": "Los riesgos de liquidez identificados son...",
  "sources": [
    {
      "content": "Fragmento relevante del documento...",
      "page": 5,
      "score": 0.89,
      "metadata": {"document": "Doc chatbot.pdf"}
    }
  ],
  "metadata": {
    "mode_used": "baseline",
    "latency_ms": 87,
    "chunks_retrieved": 5,
    "fallback_triggered": false
  }
}
```

### `POST /ingest`

Procesa nuevos documentos PDF.

**Request:**
```bash
curl -X POST http://localhost:8000/ingest \
  -F "file=@documento.pdf"
```

### `GET /health`

Verifica estado del sistema.

**Response:**
```json
{
  "status": "healthy",
  "mode": "baseline",
  "index_loaded": true,
  "documents_count": 1
}
```

---

## ğŸ¯ Roadmap

### âœ… Completado
- [x] Sistema RAG baseline funcional
- [x] API REST con FastAPI
- [x] Frontend React
- [x] Modo LLM con fallback
- [x] DocumentaciÃ³n completa

### ğŸš§ En Progreso
- [ ] DockerizaciÃ³n completa
- [ ] Tests de integraciÃ³n (>80% coverage)
- [ ] CI/CD pipeline

### ğŸ“… Futuro
- [ ] Soporte multi-documento (colecciones)
- [ ] Sistema de evaluaciÃ³n automÃ¡tica (RAGAS)
- [ ] Panel administrativo
- [ ] AutenticaciÃ³n y permisos
- [ ] CachÃ© de consultas frecuentes
- [ ] Soporte para mÃ¡s formatos (DOCX, TXT, HTML)
- [ ] IntegraciÃ³n con S3 para almacenamiento
- [ ] MÃ©tricas y observabilidad (Prometheus + Grafana)

---

## ğŸ“Š AnÃ¡lisis del Proyecto

### âœ… Fortalezas

**1. DiseÃ±o de IngenierÃ­a Robusto**
- La estrategia baseline-first es **excepcional** para producciÃ³n
- SeparaciÃ³n clara de responsabilidades (baseline vs LLM)
- Fallback automÃ¡tico garantiza alta disponibilidad

**2. Pragmatismo TÃ©cnico**
- Evita over-engineering comÃºn en proyectos RAG
- BM25 es subestimado pero altamente efectivo para bÃºsqueda lÃ©xica
- Enfoque en reproducibilidad y trazabilidad (crÃ­tico en riesgos)

**3. Arquitectura Escalable**
- Backend y frontend desacoplados
- FÃ¡cil agregar nuevos retriever strategies
- Preparado para multi-tenancy

### âš ï¸ Ãreas de Mejora

**1. Testing**
```python
# Sugerencia: Agregar tests unitarios
tests/
â”œâ”€â”€ test_baseline_rag.py
â”œâ”€â”€ test_pdf_loader.py
â””â”€â”€ test_query_wrapper.py
```

**2. ConfiguraciÃ³n**
- Centralizar configuraciÃ³n en `config.py` o Pydantic Settings
- ValidaciÃ³n de variables de entorno al inicio

**3. Observabilidad**
```python
# Agregar logging estructurado
import structlog
logger = structlog.get_logger()

@app.post("/query")
async def query(q: QueryRequest):
    logger.info("query_received", question=q.question, mode=q.mode)
    # ...
```

**4. Seguridad**
- SanitizaciÃ³n de inputs (prevenir injection en consultas)
- Rate limiting en endpoints
- CORS configurado correctamente

**5. EvaluaciÃ³n**
```python
# Sistema de evaluaciÃ³n automÃ¡tica
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy

# Evaluar calidad de respuestas
results = evaluate(
    dataset=test_dataset,
    metrics=[faithfulness, answer_relevancy]
)
```

### ğŸ“ Mejores PrÃ¡cticas Aplicadas

âœ… **Separation of Concerns**: Cada mÃ³dulo tiene responsabilidad Ãºnica  
âœ… **Fail-Safe Design**: Sistema funciona incluso sin LLM  
âœ… **Explicit over Implicit**: Fuentes y metadata siempre visibles  
âœ… **Production-First**: No es un notebook, es un sistema  

### ğŸ’¡ Recomendaciones Finales

**Corto Plazo (1-2 semanas):**
1. Agregar tests con pytest (target: >70% coverage)
2. Dockerizar completamente
3. Implementar rate limiting

**Mediano Plazo (1-2 meses):**
1. Sistema de evaluaciÃ³n automÃ¡tica
2. Soporte multi-documento
3. CachÃ© con Redis

**Largo Plazo (3-6 meses):**
1. Dashboard administrativo
2. Sistema de feedback de usuarios
3. A/B testing baseline vs LLM

---

## ğŸ† ConclusiÃ³n

Este es un **excelente ejemplo de ingenierÃ­a de IA aplicada**. No persigue el hype de LLMs sino que construye un sistema **confiable, auditable y mantenible**.

### PuntuaciÃ³n Global: **8.5/10**

**Desglose:**
- Arquitectura: 9/10
- CÃ³digo: 8/10
- DocumentaciÃ³n: 9/10
- Testing: 6/10 (Ã¡rea principal de mejora)
- ProducciÃ³n-Ready: 8/10

**Veredicto:** Proyecto sÃ³lido con visiÃ³n clara de ingenierÃ­a. Con los ajustes sugeridos, fÃ¡cilmente alcanza 9.5/10 y estÃ¡ listo para entornos enterprise.

---

## ğŸ‘¤ Autor

Proyecto desarrollado como ejercicio aplicado de **ingenierÃ­a de sistemas RAG**, con Ã©nfasis en robustez, reproducibilidad y diseÃ±o industrial de IA.

**FilosofÃ­a:** *"El mejor sistema de IA es el que funciona cuando la API de OpenAI estÃ¡ caÃ­da"*

---

## ğŸ“„ Licencia

[MIT](LICENSE)

---

## ğŸ¤ ContribuciÃ³n

Pull requests bienvenidos. Para cambios mayores, por favor abrir un issue primero.
```bash
# Fork y clonar
git checkout -b feature/nueva-funcionalidad
git commit -m "Agrega nueva funcionalidad"
git push origin feature/nueva-funcionalidad
```

---

**Â¿Preguntas?** Abre un issue o contacta al equipo.
