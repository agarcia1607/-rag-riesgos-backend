import os
import logging
from pathlib import Path
from typing import Any, Dict

from dotenv import load_dotenv

# âœ… Imports internos como paquete "backend"
from backend.baseline_rag import BaselineRAG

# (LLM opcional)
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def _default_pdf_path() -> str:
    """
    Devuelve la ruta absoluta al PDF dentro de /data, independientemente
    de desde dÃ³nde se ejecute el programa.
    Estructura esperada:
      repo_root/
        data/Doc chatbot.pdf
        backend/query_wrapper.py
    """
    repo_root = Path(__file__).resolve().parents[1]
    return str(repo_root / "data" / "Doc chatbot.pdf")


class ChatbotRiesgos:
    """
    Chatbot RAG para consultas sobre riesgos.

    Modos:
    - baseline: BM25 + extractivo (sin tokens, reproducible)
    - llm: Gemini + Chroma (opcional, requiere API key)

    Control de modo:
    - Variable de entorno RAG_MODE = "baseline" | "llm"
      Si no estÃ¡ definida, selecciona automÃ¡ticamente.
    """

    def __init__(self, persist_directory: str = "chroma_db_riesgos", temperature: float = 0.3, model: str = "gemini-1.5-flash"):
        self.persist_directory = persist_directory
        self.temperature = temperature
        self.model = model

        self.vectorstore = None
        self.qa_chain = None
        self.baseline = None
        self.embedding_function = None
        self.llm = None

        # Cargar variables de entorno
        load_dotenv()
        self.google_api_key = os.getenv("GOOGLE_API_KEY")

        # âœ… Ruta robusta al PDF
        self.pdf_path = _default_pdf_path()

        # ğŸ”‘ SelecciÃ³n de modo (forzado o automÃ¡tico)
        forced_mode = os.getenv("RAG_MODE", "").strip().lower()  # "baseline" o "llm"
        if forced_mode in {"baseline", "llm"}:
            self.mode = forced_mode
        else:
            # Auto: si hay API key y existe el vectorstore -> llm, si no -> baseline
            if self.google_api_key and Path(self.persist_directory).exists():
                self.mode = "llm"
            else:
                self.mode = "baseline"

        logger.info(f"ğŸ§© Modo seleccionado: {self.mode}")

        # ğŸŸ¢ BASELINE (sin tokens)
        if self.mode == "baseline":
            self.baseline = BaselineRAG(pdf_path=self.pdf_path, debug=False)
            logger.info("âœ… Baseline inicializado (BM25 + extractivo).")
            return

        # ğŸ”µ LLM (opcional)
        self._setup_components()

    def _setup_components(self):
        """Configura los componentes del modo LLM (Gemini + Chroma)."""
        try:
            logger.info("ğŸ”§ Inicializando embeddings...")
            self.embedding_function = GoogleGenerativeAIEmbeddings(
                model="models/embedding-001",
                google_api_key=self.google_api_key
            )

            logger.info("ğŸ“š Cargando vector store...")
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embedding_function
            )

            logger.info(f"ğŸ¤– Inicializando modelo Gemini: {self.model}")

            modelos_disponibles = [
                "gemini-1.5-flash",
                "gemini-1.5-pro",
                "gemini-2.0-flash",
                "gemini-2.5-flash",
                "gemini-2.5-pro",
            ]
            if self.model not in modelos_disponibles:
                logger.warning(f"âš ï¸ Modelo {self.model} no reconocido. Usando gemini-1.5-flash por defecto.")
                self.model = "gemini-1.5-flash"

            self.llm = ChatGoogleGenerativeAI(
                model=self.model,
                google_api_key=self.google_api_key,
                temperature=self.temperature
            )

            self.prompt_template = PromptTemplate(
                template=(
                    "Eres un asistente experto en anÃ¡lisis de riesgos. "
                    "Responde de manera clara y precisa basÃ¡ndote en la informaciÃ³n proporcionada.\n\n"
                    "Contexto: {context}\n\n"
                    "Pregunta: {question}\n\n"
                    "Respuesta detallada:"
                ),
                input_variables=["context", "question"]
            )

            logger.info("ğŸ”— Configurando cadena QA...")
            retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 5}
            )

            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True,
                chain_type_kwargs={"prompt": self.prompt_template}
            )

            logger.info("âœ… Modo LLM inicializado correctamente.")

        except Exception as e:
            # ğŸ”¥ En vez de romper el sistema, degradamos a baseline
            logger.error(f"âŒ Error al inicializar componentes LLM: {e}")
            logger.info("â†©ï¸ Fallback automÃ¡tico a baseline (sin tokens).")

            self.mode = "baseline"
            self.baseline = BaselineRAG(pdf_path=self.pdf_path, debug=False)

            self.vectorstore = None
            self.qa_chain = None
            self.embedding_function = None
            self.llm = None

            logger.info("âœ… Baseline inicializado tras fallo en LLM.")

    def consultar(self, pregunta: str, mostrar_fuentes: bool = False) -> Dict[str, Any]:
        """
        Realiza una consulta al sistema.

        Returns:
            dict: {"respuesta": str, "fuentes": list}
        """
        try:
            logger.info(f"ğŸ” Procesando consulta: {pregunta[:50]}...")

            # âœ… Baseline
            if getattr(self, "mode", "llm") == "baseline":
                return self.baseline.ask(pregunta)

            # âœ… LLM
            if self.qa_chain is None:
                raise RuntimeError("qa_chain no estÃ¡ inicializada en modo llm.")

            respuesta = self.qa_chain.invoke({"query": pregunta})

            resultado = {
                "respuesta": respuesta["result"],
                "fuentes": respuesta.get("source_documents", [])
            }

            if mostrar_fuentes and resultado["fuentes"]:
                logger.info(f"ğŸ“„ Encontradas {len(resultado['fuentes'])} fuentes relevantes")

            return resultado

        except Exception as e:
            logger.error(f"âŒ Error al procesar consulta (modo {getattr(self, 'mode', '?')}): {e}")

            # ğŸ” Si fallÃ³ LLM (429/cuota/etc.), degradar a baseline automÃ¡ticamente
            if getattr(self, "mode", "llm") == "llm":
                logger.info("â†©ï¸ Fallback a baseline por error en LLM.")
                try:
                    self.mode = "baseline"
                    if self.baseline is None:
                        self.baseline = BaselineRAG(pdf_path=self.pdf_path, debug=False)
                    return self.baseline.ask(pregunta)
                except Exception as e2:
                    logger.error(f"âŒ TambiÃ©n fallÃ³ baseline: {e2}")

            return {"respuesta": f"âŒ Error al procesar la consulta: {str(e)}", "fuentes": []}

    def buscar_documentos_similares(self, consulta: str, k: int = 3):
        """
        Busca documentos similares sin generar respuesta.
        Funciona en baseline y en llm.
        """
        try:
            # âœ… Baseline: devolvemos los chunks mÃ¡s relevantes del BM25
            if getattr(self, "mode", "llm") == "baseline":
                hits = self.baseline.store.search(consulta, k=k)  # [(Chunk, score), ...]
                return [chunk for (chunk, _score) in hits]

            # âœ… LLM: bÃºsqueda semÃ¡ntica
            if self.vectorstore is None:
                raise RuntimeError("vectorstore no estÃ¡ inicializado en modo llm.")
            return self.vectorstore.similarity_search(consulta, k=k)

        except Exception as e:
            logger.error(f"âŒ Error en bÃºsqueda de similitud: {e}")
            return []

    def mostrar_fuentes(self, fuentes):
        """Muestra las fuentes de informaciÃ³n de manera formateada."""
        if not fuentes:
            print("ğŸ“„ No se encontraron fuentes especÃ­ficas.")
            return

        print(f"\nğŸ“š Fuentes consultadas ({len(fuentes)}):")
        print("-" * 50)

        for i, doc in enumerate(fuentes, 1):
            content = getattr(doc, "page_content", None)
            if content is None:
                # baseline chunk
                content = getattr(doc, "text", str(doc))

            contenido = content[:200] + "..." if len(content) > 200 else content
            print(f"{i}. {contenido}")

            meta = getattr(doc, "metadata", None)
            if meta:
                print(f"   ğŸ“‹ Metadata: {meta}")
            print()
